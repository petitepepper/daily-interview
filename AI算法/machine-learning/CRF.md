[TOC]

# 基础知识

## 1. 概率图模型

如果用一个词来形容概率图模型（Probabilistic Graphical Model）的话，那就是“优雅”。对于一个实际问题，我们希望能够挖掘隐含在数据中的知识。概率图模型构建了这样一幅图，用观测结点表示观测到的数据，用隐含结点表示潜在的知识，用边来描述知识与数据的相互关系，**最后基于这样的关系图获得一个概率分布**，非常“优雅”地解决了问题。

概率图模型包括了朴素贝叶斯模型、最大熵模型、隐马尔可夫模型、条件随机场、主题模型等。

- **有向无环**(Directed Acyclic Graphical) —— 表示**因果**关系 ： 有向图模型 / 贝叶斯网络（Bayesian Network）
- 无向图 —— 表**相关**关系：无向图模型 / 马尔可夫网络（Markov Network），马尔科夫随机场（Markov Random Field）

## 2. [马尔可夫随机场](https://www.cnblogs.com/jiangxinyang/p/9309742.html) (MRF)

马尔可夫随机场属于概率图模型，是典型的马尔可夫网，是一种无向图的生成模型。

<img src="https://img-blog.csdn.net/20180927174323799?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hvaGFpeng=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img" style="zoom:33%;" />

- 每个结点表示一个或者一组变量
- 结点之间的边表示两个变量之间的依赖关系

在马尔科夫随机场中存在一组势函数（定义在变量子集上的非负实函数），也称为因子，主要是用于定义概率分布函数。

概率图模型就是由图表示的概率分布（准确的说就是表示**各变量之间的依赖关系**）。

设有联合概率分布P(Y) 是一组随机变量，给出表示它的无向图G。则这组随机变量位于图中的各结点上。在这里我们引入三个概念：成对马尔科夫性、局部马尔科夫性、全局马尔科夫性，用来表示无向图中随机变量之间的关系。



### 2.1 势函数

势函数`Φ`的作用是定量刻画变量级XQ中变量之间的相关关系。它应该是非负函数,且在所偏好的变量取值上有较大的函数值。

例如，对于下图

<img src="https://upload-images.jianshu.io/upload_images/4155986-50f34b9956bdf6b5.png?imageMogr2/auto-orient/strip|imageView2/2/w/438/format/webp" alt="img" style="zoom:50%;" />

其势函数是下面的样子：

<img src="https://upload-images.jianshu.io/upload_images/4155986-9c80007ce1bd6fcf.png?imageMogr2/auto-orient/strip|imageView2/2/w/618/format/webp" alt="img" style="zoom:50%;" />

则说明该模型偏好变量$X_A$和$X_C$拥有相同取值，$X_B$和$X_C$拥有不同的取值。

为了满足非负性，指数函数常被用于定义势函数：

![img](https://upload-images.jianshu.io/upload_images/4155986-9e1b9ee513c6e67f.png?imageMogr2/auto-orient/strip|imageView2/2/w/1146/format/webp)





### 2.2 条件独立性

> **马尔可夫性**：当一个随机过程在给定现在状态及所有过去状态情况下，其**未来状态的条件概率分布仅依赖于当前状态**；换句话说，在给定现在状态时，它与过去状态（即该过程的历史路径）是条件独立的，那么此随机过程即具有马尔可夫性质。



1. **成对马尔科夫性**

   在无向图`G`中：

   - 两个没有边连接的结点`v1`, `v2`，它们对应的随机变量分别为$Y_{v1}$,  $Y_{v2}$
   - 其他所有节点为`O`，对应随机变量为$Y_O$

   则 $P(Y_{v1},Y_{v2}|Y_O) =P(Y_{v1}|Y_O)·P(Y_{v1}|Y_O) $

   即，在给定了随机变量组$Y_O$的条件下随机变量$Y_{v1}$和 $Y_{v2}$是条件独立的。

   

2. **局部马尔科夫性**

   ![img](https://pic1.zhimg.com/80/v2-e965e5b707b66f411d3616584d87a8b8_1440w.jpg)

   - `v`是无向图G中的任意结点 <font color='lightgray'>如图中红色</font>
   - `W`是与`v`有边连接的所有结点 <font color='lightgray'>如图中灰色</font>
   - `O`是`v`，`W`以外的所有结点（相当于`W`将`v`和`O`给隔开了 <font color='lightgray'>如图中白色</font>）

   则在给定`W`的条件下，`v`和`O`之间是相互独立的，具体表达式如下： $P(Y_{v},Y_{O}|Y_W) =P(Y_{v}|Y_W)·P(Y_{O}|Y_W) $

   

3. **全局马尔科夫性**

   设结点集`A`，`B`在无向图G中被结点集合`C`分开的任意结点的集合

   <img src="https://pic4.zhimg.com/80/v2-e4286d5db747992f806db40757275e5f_1440w.jpg" alt="img" style="zoom:67%;" />

   则在给定了集合C的条件下结点集合A和B之间是相互独立的，具体表达式如下： $P(Y_{A},Y_{B}|Y_C) =P(Y_{A}|Y_C)·P(Y_{B}|Y_C) $



这三种性质实质上是等价的，成对马尔科夫性和局部马尔科夫性都可以看作是全局马尔科夫性的特殊形式。

那这三种性质提出来有什么用呢？首先满足这三种性质的联合概率分布P(Y)可以称为<u>马尔科夫随机场</u> or <u>概率无向图模型</u>。而对于马尔科夫随机场，**可以将联合概率分布P(Y)拆分成多个因子的乘积**，这样就便于计算P(Y)



### 2.3 模型的因子分解

两个定义：

- `团` ：一个子集中，任意两个节点之间都有边连接
- `最大团`：最大团就是不能被其他团包含的团（对于一个团，当加入一个新的结点之后无法构成团，则称为最大团）

团 <=> 势函数

> 例子：
>
> ![img](https://images2018.cnblogs.com/blog/1335117/201807/1335117-20180714150542102-300486166.png)
>
> 对于上图中的无向图模型，团的个数有五个：{Y1, Y2}，{Y2, Y3}，{Y3, Y4}，{Y4, Y2}，{Y1, Y3}。
>
> 最大团的个数有两个：{Y1, Y2, Y3}，{Y2, Y3, Y4}

对于联合概率分布P(Y)，可以将它分解为**最大团上**的**随机变量的函数**的乘积，这就是概率无向图模型的因子分解。

对于给定的无向图G，`C`为G上的最大团，$Y_C$表示`C`对应的随机变量。那么联合概率分布可以表示为

![img](https://images2018.cnblogs.com/blog/1335117/201807/1335117-20180714151202674-1942267071.png)

![img](https://images2018.cnblogs.com/blog/1335117/201807/1335117-20180714151218662-345718068.png)

在这里函数$Ψ_C(Y_C)$就是最大团C上的势函数。

引入规范场因子`Z`是为了保证概率P(Y)构成一个概率分布。势函数要求是严格正的，因此通常一般定义为指数函数

![img](https://images2018.cnblogs.com/blog/1335117/201807/1335117-20180714151502725-558534852.png)

因此对于满足马尔科夫随机场的联合概率分布可以通过因子分解的形式求解。





# 条件随机场面试题


## 1. 简单介绍条件随机场 ##

> [如何轻松愉快地理解条件随机场（CRF）](https://zhuanlan.zhihu.com/p/104562658)：结合生活场景介绍了CRF，特征函数，同时和LR以及HMM做了比较

条件随机场（Conditional Random Field，简称 CRF）是给定一组输入随机变量的<u>条件下</u>，求另一组输出随机变量的条件概率分布模型。其特点是**假设输出随机变量构成马尔可夫随机场**。

CRF是一种判别式机率模型，是随机场的一种，常用于<u>标注或分析序列资料，如自然语言文字或是生物序列</u>。

如同马尔可夫随机场，条件随机场为无向图模型，图中的顶点代表随机变量，顶点间的连线代表随机变量间的相依关系，在条件随机场当中，随机变量 Y 的分布为条件机率，给定的观察值则为随机变量 X。   

原则上，条件随机场的图模型布局是可以任意给定的，一般**常用的布局是链接式**的架构，链接式架构不论在训练（training）、推论（inference）、或是解码（decoding）上，都存在有效率的算法可供演算。
条件随机场跟隐马尔可夫模型常被一起提及，条件随机场对于输入和输出的机率分布，没有如隐马尔可夫模型那般强烈的假设存在 [补充：因为HMM模型假设后面状态和前面无关]。

 

## 2. CRF预测的维特比算法求解过程

输入：模型特征向量F(y,x)和权值向量w，观测序列$x=(x_1,x_2,…,x_n)$;  
输出：最优路径$y^*=(y_1^*,y_2^*,…,y_n^*) $

初始化：
$$
\delta_{1}(j)=w \cdot F_{1}\left(y_{0}=\operatorname{start}, y_{1}=j, x\right), \quad j=1,2, \cdots, m
$$
递推：
$$
\delta_{i}(l)=\max _{1<j<m}\left\{\delta_{i-1}(j)+w \cdot F_{i}\left(y_{i-1}=j, y_{i}=l, x\right)\right\}, \quad l=1,2, \cdots, m
$$

$$
\Psi_{i}(l)=\arg \max _{1 \leqslant j \leqslant m}\left\{\delta_{t-1}(j)+w \cdot F_{i}\left(y_{i-1}=j, y_{i}=l, x\right)\right\}, \quad l=1,2, \cdots, m
$$

终止：
$$
\max _{y}(w \cdot F(y, x))=\max _{1<j<m} \delta_{n}(j)
$$

$$
y_{n}^{*}=\arg \max _{1 \leqslant j \leqslant m} \delta_{n}(j)
$$

返回路径:
$$
y_{i}^{*}=\Psi_{i+1}\left(y_{i+1}^{*}\right), \quad i=n-1, n-2, \cdots, 1
$$

##3. 链式条件随机场[chain-structured CRF]条件概率公式：  

$$
P(\mathbf{y} \mid \mathbf{x})=\frac{1}{Z} \exp \left(\sum_{j} \sum_{i=1}^{n-1} \lambda_{j} t_{j}\left(y_{i+1}, y_{i}, \mathbf{x}, i\right)+\sum_{k} \sum_{i=1}^{n} \mu_{k} s_{k}\left(y_{i}, \mathbf{x}, i\right)\right)
$$




## 3. HMM、MEMM和CRF模型的比较  

* HMM模型是对转移概率（隐藏状态转移到隐藏状态的概率）和表现概率（隐藏状态到观察状态的概率）直接建模，统计共现概率；
* MEMM模型是对转移概率和表现概率建立联合概率，统计时统计的是条件概率，而非共现概率。MEMM容易陷入局部最优，主要因为是MEMM只在局部做归一化；
* CRF模型则统计的是全局概率，在归一化时考虑了数据在全局的分布，而不仅仅是局部归一化，这样也就解决了MEMM中的标记偏置问题；



## 4. 注意要点  



### 4.1 贝叶斯网络和MRF的分解计算问题  

贝叶斯网络中每个节点都对应一个先验概率分布或者条件概率分布，因此整体联合概率分布可以直接分解为所有单个节点分布的乘积。
对于马尔科夫随机场，由于变量间没有明确的因果关系，它的联合概率分布通常会表达为一系列势函数[Potential Function]的乘积，因为乘积之和通常不为1，所以要进行归一化才能成为一个有效的概率分布。  



### 4.2 影响概率图模型学习精度的因素

- 语料库样本集对总体的代表性；  
- 模型算法理论基础及所针对的问题。不同模型的理论不同，所擅长处理的NLP任务也不同，比如：朴素贝叶斯模型处理短文本分类效果很好，最大熵模型在处理中文词性标注表现很好，条件随机场处理中文分词，语义组块等方便精度很好，Semi-CRF在处理命名实体识别精度很好。  
- 模型算法的复杂度。属于工程问题，一般讲，要求模型参数估计的越精确，模型复杂度越高，学习时间越长，推断和预测的精度也越高。  



### 4.3 Bi-LSTM-CRF算法解析  

![image-20210903204605132](img/CRF/image-20210903204605132.png)

 Bi-LSTM-CRF模型的输入是每个单词的词向量，经过双向LSTM层提取特征并输出为5个label的得分，再将该得分输入进CRF层，得到这句话最终最大可能的识别标签。因为BiLSTM层得到的label并不总是满足实际情况，CRF层能够添加一些约束使得预测标签是有效的。这些约束便是从训练数据的过程中学习得到的。



### 4.4 常见的概率图模型中的生成/判别模型

> **生成式**模型是对联合概率分布$P(X,Y,Z)$进行建模，在给定观测集合X的条件下，通过计算 边缘分布来得到对变量集合Y的推断，即

$$
  P(Y \mid X)=\frac{P(X, Y)}{P(X)}=\frac{\sum_{Z} P(X, Y, Z)}{\sum_{Y . Z} P(X, Y, Z)}
$$

  

> **判别式**模型是直接对条件概率分布$P(Y,Z|X)$进行建模，然后消掉无关变量Z就可以 得到对变量集合Y的预测，即:

$$
  P(Y \mid X)=\sum_{Z} P(Y, Z \mid X)
$$

常见的概率图模型有：朴素贝叶斯、最大熵模型、贝叶斯网络、隐马尔可夫模型、条件随机场、pLSA、LDA等。

基于前面的问题解答，我们可以知道

- 生成式模型：

  ①朴素贝叶斯、贝叶斯网络、pLSA、LDA等模型

  它们都是先对联合概率分布进行建模，然后再通过计算边缘分布得到对变量的预测

  ②隐马尔可夫模型 （对序列数据进行建模的方法）

- 判别式模型：最大熵模型、条件随机场（对序列数据进行建模的方法）

  直接对条件概率分布进行建模

  





## 参考  

1.条件随机场定义参考维基百科  
2.Bi-LSTM-CRF算法解析参考: https://createmomo.github.io/  
3.数学之美 - 吴军  
4.百面机器学习 - 诸葛越&葫芦娃   
5.NLP汉语自然语言处理原理与实践 - 郑捷  
6.http://blog.sina.com.cn/s/blog_6d1875160101gy4e.html