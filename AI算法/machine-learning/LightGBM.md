![](img/LightGBM/LightGBM.PNG)

[TOC]

# LightGBM面试题

## 1. 简单介绍一下LightGBM？

从 LightGBM 名字我们可以看出其是轻量级（Light）的梯度提升机（GBM），其相对 XGBoost 具有训练速度快、内存占用低的特点。

LightGBM 是为解决GBDT训练速度慢，内存占用大的缺点，此外还提出了：

- 基于Histogram的决策树算法

- 单边梯度采样 Gradient-based One-Side Sampling(GOSS)

- 互斥特征捆绑 Exclusive Feature Bundling(EFB)

- 带深度限制的Leaf-wise的叶子生长策略

- 直接支持类别特征(Categorical Feature)

- 支持高效并行

- Cache命中率优化

## 2. 介绍一下直方图算法？

直方图算法就是使用直方图统计，将大规模的数据放在了直方图中，对每一个特征，记录各个bin中**样本的梯度之和**，以及**样本数量**

- 首先确定对于每一个特征需要多少个箱子并为每一个箱子分配一个整数

- 将浮点数的范围均分成若干区间，区间个数与箱子个数相等

- 将属于该箱子的样本数据更新为箱子的值

- 最后用直方图表示

优点：

**内存占用更小**：相比xgb不需要额外存储预排序，且只保存特征离散化后的值(整型)

**计算代价更小**: 相比xgb不需要遍历一个特征值就需要计算一次分裂的增益，只需要计算k次(k为箱子的个数)

> 当然，Histogram算法并不是完美的。由于特征被离散化后，找到的并不是很精确的分割点，所以会对结果产生影响。但在不同的数据集上的结果表明，<u>离散化的分割点对最终的精度影响并不是很大，甚至有时候会更好一点</u>，这是因为：
>
> - 决策树本来就是弱模型，分割点是不是精确并不是太重要；
> - 较粗的分割点也有正则化的效果，可以有效地防止过拟合；
> - 即使单棵树的训练误差比精确分割的算法稍大，但在梯度提升（Gradient Boosting）的框架下没有太大的影响

**直方图做差加速**：一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到，在速度上可以提升一倍

<img src="https://pic4.zhimg.com/80/v2-b51f2764c13ca0a7b4cb41849a367a87_1440w.jpg" alt="img" style="zoom:67%;" />

> **注意：**XGBoost 在进行预排序时只考虑<u>非零值</u>进行加速，而 LightGBM 也采用类似策略：只用非零特征构建直方图。



## 3. 介绍一下Leaf-wise和 Level-wise？

|      | Level-wise                                                   | Leaf-wise                                                    |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
|      | XGBoost                                                      | LightGBM                                                     |
| 过程 | 遍历一次数据可以同时分裂同一层的叶子                         | 每次从当前所有叶子中，<u>找到分裂增益最大的一个叶子，然后分裂</u>，如此循环 |
| 优点 | 1. 容易进行多线程优化<br />2. 容易控制模型复杂度，不容易过拟合 | 1. 相同分裂次数下，可以更好地降低误差，得到更好的精度        |
| 缺点 | <u>**不加区分的对待同一层的叶子**</u>，实际上很多叶子的分裂增益较低，没必要进行搜索和分裂 | 可能会长出比较深的决策树，产生**<u>过拟合</u>**              |

XGBoost 采用 Level-wise，策略遍历一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效的算法，因为它<u>**不加区分的对待同一层的叶子**</u>，实际上很多叶子的分裂增益较低，没必要进行搜索和分裂

LightGBM采用Leaf-wise的增长策略，该策略每次从当前所有叶子中，<u>找到分裂增益最大的一个叶子，然后分裂</u>，如此循环。因此同Level-wise相比，Leaf-wise的优点是：在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度；Leaf-wise的缺点是：可能会长出比较深的决策树，产生过拟合。因此LightGBM会在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合

## 4. 介绍一下单边梯度采样算法(GOSS)？

GOSS算法从**<font color='green'><u>减少样本</u></font>**的角度出发，排除大部分小梯度的样本，仅用剩下的样本计算信息增益，它是一种在减少数据量和保证精度上平衡的算法。

与此同时，为了不改变数据的总体分布，GOSS对要进行分裂的特征按照绝对值大小进行排序，选取最大的a*100%个数据，在剩下梯度小的数据中选取b\*100%个，这b\*100%个数据乘以权重$\frac{1-a}{b}$,最后使用这(a+b)\*100%个数据计算信息增益。

> 这样算法就会关注到训练不足的样本，而不会过多改变原数据集的分布

## 5. 介绍互斥特征捆绑算法(EFB)？

高维度的数据往往是稀疏的，这种稀疏性启发我们设计一种无损的方法来减少特征的维度。

- 通常被捆绑的特征都是互斥的（**即特征不会同时为非零值**，像one-hot），这样两个特征捆绑起来才不会丢失信息
- 如果两个特征并不是完全互斥（部分情况下两个特征都是非零值）：可以用一个指标对特征不互斥程度进行衡量，称之为冲突比率，当这个值较小时，我们可以选择把不完全互斥的两个特征捆绑，而不影响最后的精度

互斥特征捆绑算法（Exclusive Feature Bundling, EFB）指出如果将一些特征进行融合绑定，则可以降低特征数量。这样在构建直方图时的时间复杂度从$O(\#data*\#feature)$ 变为  $O(\#data*\#bundle)$ ，这里 $\#bundle$指特征融合绑定后特征包的个数，且 $\#bundle$远小于$\#feature$ 。

### 5.1 捆绑步骤

> LightGBM的EFB算法将这个问题转化为图着色的问题来求解，将所有的特征视为图的各个顶点，将不是相互独立的特征用一条边连接起来，边的权重就是两个相连接的特征的总冲突值，这样需要绑定的特征就是在图着色问题中要涂上同一种颜色的那些点（特征）。另外，算法可以允许一小部分的冲突，我们可以得到更少的绑定特征，进一步提高计算效率。

1. 构造一个加权无向图，顶点是特征，边有权重，其权重与两个特征间冲突相关；
2. 根据节点的度进行降序排序，度越大，与其它特征的冲突越大；
3. 遍历每个特征，将它分配给现有特征包，或者新建一个特征包，使得总体冲突最小。

算法允许两两特征并不完全互斥来增加特征捆绑的数量，通过设置最大冲突比率 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 来平衡算法的精度和效率。EFB 算法的伪代码如下所示：

<img src="https://pic1.zhimg.com/80/v2-1b2636a948ece17fae81be7f400fedfc_1440w.jpg" alt="img" style="zoom:50%;" />

### 5.2. 特征之间如何捆绑？

特征合并算法，其关键在于原始特征能从合并的特征中分离出来。绑定几个特征在同一个bundle里需要保证绑定前的原始特征的值可以在bundle中识别，考虑到histogram-based算法将连续的值保存为离散的bins，我们可以使得不同特征的值分到bundle中的不同bin（箱子）中，这可以通过在特征值中加一个偏置常量来解决。

> 比如，我们在bundle中绑定了两个特征A和B，A特征的原始取值为区间 $[0,10)$，B特征的原始取值为区间$[0,20)$，我们可以在B特征的取值上加一个偏置常量10，将其取值范围变为$[10,30)$，绑定后的特征取值范围为$[0,30)$



## 6. LightGBM是怎么支持类别特征？

[决策树中不推荐使用one-hot的原因(见3.1小节)](https://zhuanlan.zhihu.com/p/99069186)

LightGBM优化了对类别特征的支持，**可以直接输入类别特征**，不需要额外的0/1展开。LightGBM采用 many-vs-many 的切分方式将类别特征分为两个子集，实现类别特征的最优切分。假设某维特征有 k 个类别，则有 2^k-1^-1 种可能，时间复杂度为 O(2^k^)，LightGBM 基于 Fisher的论文实现了 O(klogk) 的时间复杂度

* 离散特征建立直方图的过程 

  统计该特征下每一种离散值出现的次数，并从高到低排序，并过滤掉出现次数较少的特征值, 然后为每一个特征值，建立一个bin容器。

  <img src="https://pic4.zhimg.com/v2-d82a2e30ed932dcc38356c125004d247_r.jpg" alt="preview" style="zoom:40%;" />

* 计算分裂阈值的过程 

  * 先看该特征下划分出的bin容器的个数，如果bin容器的数量小于4，直接使用one vs others方式, 逐个扫描每一个bin容器，找出最佳分裂点;

  * 对于bin容器较多的情况, 先进行过滤，只让子集合较大的bin容器参加划分阈值计算, 对每一个符合条件的bin容器进行公式计算
    $$
    \frac{该bin容器下所有样本的一阶梯度之和 }{ 该bin容器下所有样本的二阶梯度之和} + 正则项 
    $$
  
* **这里为什么不是label的均值呢？其实"label的均值"只是为了便于理解，只针对了学习一棵树且是回归问题的情况， 这时候一阶导数是Y, 二阶导数是1**)，得到一个值，根据该值对bin容器从小到大进行排序，然后分从左到右、从右到左进行搜索，得到最优分裂阈值。但是有一点，没有搜索所有的bin容器，而是设定了一个搜索bin容器数量的上限值，程序中设定是32，即参数max_num_cat。
  
* LightGBM中对离散特征实行的是many vs many 策略，这32个bin中最优划分的阈值的左边或者右边所有的bin容器就是一个many集合，而其他的bin容器就是另一个many集合。
  
* 对于连续特征，划分阈值只有一个，对于离散值可能会有多个划分阈值，每一个划分阈值对应着一个bin容器编号，当使用离散特征进行分裂时，只要数据样本对应的bin容器编号在这些阈值对应的bin集合之中，这条数据就加入分裂后的左子树，否则加入分裂后的右子树。

## 7. LightGBM的优缺点

优点：

- 直方图算法极大的降低了时间复杂度；
- 单边梯度算法过滤掉梯度小的样本，减少了计算量；
- 基于 Leaf-wise 算法的增长策略构建树，减少了计算量；
- 直方图算法将存储特征值转变为存储 bin 值，降低了内存消耗
- 互斥特征捆绑算法减少了特征数量，降低了内存消耗

缺点：

- LightGBM在Leaf-wise可能会长出比较深的决策树，产生过拟合
- LightGBM是基于偏差的算法，所以会对**噪点较为敏感**；



## 8. GBDT是如何做回归和分类的

- **回归**

  生成每一棵树的时候，第一棵树的一个叶子节点内所有样本的label的均值就是这个棵树的预测值，后面根据残差再预测，最后根据将第一棵树的预测值+权重*(其它树的预测结果)

  

* **分类**

  分类时针对样本有三类的情况，

  * 首先同时训练三颗树。
    - 第一棵树针对样本 x 的第一类，输入为（x, 0）。
    - 第二棵树输入针对样本 x 的第二类，假设 x 属于第二类，输入为（x, 1）。
    - 第三棵树针对样本 x 的第三类，输入为（x, 0）。
    - 参照 CART 的生成过程。输出三棵树对 x 类别的预测值 f1(x), f2(x), f3(x)。
  * 在后面的训练中，我们仿照多分类的逻辑回归，使用 softmax 来产生概率。
    - 针对类别 1 求出残差 f11(x) = 0 − f1(x)；
    - 类别 2 求出残差 f22(x) = 1 − f2(x)；
    - 类别 3 求出残差 f33(x) = 0 − f3(x)。
  * 然后第二轮训练，
    - 第一类输入为(x, f11(x))
    - 第二类输入为(x, f22(x))
    - 第三类输入为(x, f33(x))。
  * 继续训练出三棵树，一直迭代 M 轮，每轮构建 3 棵树。当训练完毕以后，新来一个样本 x1，我们需要预测该样本的类别的时候，便可使用 softmax 计算每个类别的概率。

  

## 参考资料

深入理解LightGBM https://mp.weixin.qq.com/s/zejkifZnYXAfgTRrkMaEww

决策树（下）——XGBoost、LightGBM（非常详细）- 知乎 https://zhuanlan.zhihu.com/p/87885678

Lightgbm如何处理类别特征： https://blog.csdn.net/anshuai_aw1/article/details/83275299

LightGBM 直方图优化算法：https://blog.csdn.net/jasonwang_/article/details/80833001
